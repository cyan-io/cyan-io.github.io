<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>所有文章 - Cyan&#39;s Notebook</title>
    <link>/posts/</link>
    <description>所有文章 | Cyan&#39;s Notebook</description>
    <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Wed, 06 Dec 2023 08:46:42 &#43;0000</lastBuildDate><atom:link href="/posts/" rel="self" type="application/rss+xml" /><item>
  <title>NP</title>
  <link>/posts/2023-12-06-np/</link>
  <pubDate>Wed, 06 Dec 2023 08:46:42 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-12-06-np/</guid>
  <description><![CDATA[Summary 1 2 3 4 5 6 7 8 ____________________________________________________________ | Problem Type | Verifiable in P time | Solvable in P time | Increasing Difficulty ___________________________________________________________| | | P | Yes | Yes | | | NP | Yes | Yes or No * | | | NP-Complete | Yes | Unknown | | | NP-Hard | Yes or No ** | Unknown *** | | ____________________________________________________________ V * An NP problem that is also P is solvable in P time.]]></description>
</item>
<item>
  <title>WSL2 Tips</title>
  <link>/posts/2023-12-06-wsl2-tips/</link>
  <pubDate>Wed, 06 Dec 2023 07:04:11 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-12-06-wsl2-tips/</guid>
  <description><![CDATA[注意，WSL2与WSL使用完全不同的实现
WSL2是如何运行的 当开启Hyper-V后，Windows系统相当于一个VM，而WSL2相当于另一个VM
WSL2与Win的文件访问效率 WSL2与Win通过TCP传输文件，相当于通过网络连接，文件互访效率大打折扣
WSL2运行CUDA程序 https://docs.nvidia.com/cuda/wsl-user-guide/index.html
WSL2运行CUDA的性能损失 根据任务负载，CUDA性能损失也会有大幅变化，见：https://developer.nvidia.com/blog/leveling-up-cuda-performance-on-wsl2-with-new-enhancements/]]></description>
</item>
<item>
  <title>conda source &amp; torch</title>
  <link>/posts/2023-11-28-conda-source--torch/</link>
  <pubDate>Mon, 27 Nov 2023 19:41:54 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-11-28-conda-source--torch/</guid>
  <description><![CDATA[Start Locally | PyTorch
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 channels: - defaults show_channel_urls: true default_channels: - https://mirrors.bfsu.edu.cn/anaconda/pkgs/main - https://mirrors.bfsu.edu.cn/anaconda/pkgs/r - https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.bfsu.edu.cn/anaconda/cloud msys2: https://mirrors.bfsu.edu.cn/anaconda/cloud bioconda: https://mirrors.bfsu.edu.cn/anaconda/cloud menpo: https://mirrors.bfsu.edu.cn/anaconda/cloud pytorch: https://mirrors.bfsu.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.bfsu.edu.cn/anaconda/cloud simpleitk: https://mirrors.bfsu.edu.cn/anaconda/cloud deepmodeling: https://mirrors.bfsu.edu.cn/anaconda/cloud/ anaconda | 镜像站使用帮助 | 北京外国语大学开源软件镜像站 | BFSU Open Source Mirror]]></description>
</item>
<item>
  <title>模型推理内存估算</title>
  <link>/posts/2023-11-07-llm-memory-usage/</link>
  <pubDate>Tue, 07 Nov 2023 10:48:29 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-11-07-llm-memory-usage/</guid>
  <description><![CDATA[Calc 1b = 十亿 = 1e9
1 2 &gt;&gt;&gt; 1e9/1024/1024/1024 0.9313225746154785 例: FP32 7B参数量：7 * 0.93 * (32/8)=26.04GB
REF vLLM推理性能鉴赏 - 知乎 (zhihu.com)
huggingface.co]]></description>
</item>
<item>
  <title>常用计算机工具索引</title>
  <link>/posts/2023-11-05-tools/</link>
  <pubDate>Sun, 05 Nov 2023 04:58:14 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-11-05-tools/</guid>
  <description><![CDATA[Conda Index of /anaconda/archive/ | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror
anaconda | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror
Visual Studio Code Visual Studio Code - Code Editing. Redefined
Ubuntu Ubuntu Releases
Ubuntu 源使用帮助 — USTC Mirror Help 文档
Git Git (git-scm.com)]]></description>
</item>
<item>
  <title>QEMU CPU Model</title>
  <link>/posts/2023-08-06-qemu-cpu-model/</link>
  <pubDate>Sun, 06 Aug 2023 10:03:35 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-08-06-qemu-cpu-model/</guid>
  <description><![CDATA[图片使用了smms图床，特殊网络下可能无法加载；
1 qemu-system-x86_64 -M q35 -accel kvm -cpu host -m 4096 -smp 4 -drive file=drive 发挥CPU全部性能：CPU模型与指令集 1 -cpu model 使用qemu-system-x86_64 -cpu help
我们暂时只要了解，QEMU提供这么多CPU模型，是为了便于我们迁移镜像到其他物理机，特别是不同CPU的物理机。
测试 硬件：Intel 12100（4核8线程，无E核）， DDR4 32G 3200Mhz RAM
软件：GeekBench 6.1.0
宿主机：PVE运行的QEMU Ubuntu 22.04，分配8核心、10G RAM
客户机：（嵌套虚拟化） Ubuntu 22.04，4G RAM，KVM，HOST
宿主机测试性能：单核2265、多核7711，QEMU Standard PC (i440FX + PIIX, 1996) - Geekbench
测试CPU模型对CPU性能的影响： cpu model single core multi core url host 2224 7187 https://browser.geekbench.com/v6/cpu/2157089 max 2221 7366 https://browser.geekbench.com/v6/cpu/2157146 kvm64 1149 3994 https://browser.geekbench.com/v6/cpu/2157214 其中host、与max的性能可以视为误差范围内，而kvm64无论多核还是单核，理论性能测试只达到了理论性能的50%左右；]]></description>
</item>
<item>
  <title>QEMU - accel</title>
  <link>/posts/2023-08-05-qemu-accel/</link>
  <pubDate>Sat, 05 Aug 2023 13:44:16 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-08-05-qemu-accel/</guid>
  <description><![CDATA[Intro 查看QEMU文档，对Accelerator（加速器）的介绍如下：
This is used to enable an accelerator. Depending on the target architecture, kvm, xen, hax, hvf, nvmm, whpx or tcg can be available. By default, tcg is used. If there is more than one accelerator specified, the next one is used if the previous one fails to initialize.
关于kvm, xen, hax, hvf, nvmm, whpx, tcg，以下内容总结自ChatGPT 3.5（Emmm，真好用）：
技术/软件 描述 平台支持 虚拟化方式 KVM (Kernel-based Virtual Machine) 开源虚拟化解决方案，利用Linux内核虚拟化功能，在Linux上运行虚拟机 Linux 硬件虚拟化 Xen 基于微内核架构的开源虚拟化系统，支持多种客户机操作系统 多平台（特别在x86上常用） 硬件虚拟化或直接运行 HAX (Hardware Accelerated Execution) Intel提供的硬件加速执行技术，用于Android模拟器中的虚拟机 x86主机上运行ARM虚拟机 硬件加速虚拟化 HVF (Hypervisor.]]></description>
</item>
<item>
  <title>QEMU - smp (sockets, die, core, threads)</title>
  <link>/posts/2023-08-05-qemu-smp-sockets-die-core-threads/</link>
  <pubDate>Sat, 05 Aug 2023 04:50:30 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-08-05-qemu-smp-sockets-die-core-threads/</guid>
  <description><![CDATA[图片使用了smms图床，特殊网络下可能无法加载；
1 qemu-system-x86_64 -M q35 -accel kvm -cpu host -m 4096 -smp 4 -drive file=drive 前置知识：超线程——任务管理器里展示的并不是真正的CPU 测试使用的CPU为英特尔® 酷睿™ i3-12100 处理器 (intel.cn)，根据规格表，一共有4颗核心。而在htop中，可以看到8个CPU负载
是因为操作系统以CPU支持的线程数，作为逻辑核心展示，而不是物理核心；
当CPU启用超线程后，CPU 会在每个物理内核上公开两个执行上下文，一个物理内核现在就像两个“逻辑内核”一样，可以处理不同的软件线程；
By taking advantage of idle time when the core would formerly be waiting for other tasks to complete, Intel® Hyper-Threading Technology improves CPU throughput.（协程和这个理念基本相同）
What Is Hyper-Threading? - Intel
当然，超线程技术并不能让多核性能简单的x2，理想情况下可以带来20%至30%左右的性能（吞吐量）提升。
为虚拟机分配核心 QEMU使用-smp分配核心，使用方式：
1 -smp [[cpus=]n][,maxcpus=maxcpus][,sockets=sockets][,dies=dies][,clusters=clusters][,cores=cores][,threads=threads] 可以简单理解为一颗叶子节点为线程（逻辑核心）的满二叉树：
逻辑处理器数量：
$$ n=sockets*dies*clusters*cores*threads $$
其中，maxcpus是QEMU为热插拔设置的属性，在此不展开；
参数 对应的物理对象 参数作用 sockets CPU插槽 CPU插槽数目 dies CPU晶片 每个CPU有几块承载核心的晶片 clusters 晶片上的CPU簇 / cores CPU核心 每簇包含几个CPU threads / 每个CPU核心通过超线程支持的线程数 常见写法：]]></description>
</item>
<item>
  <title>LVM, Linear, RAID[01]...</title>
  <link>/posts/2023-07-30-lvm-linear-raid01/</link>
  <pubDate>Sun, 30 Jul 2023 13:38:36 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-07-30-lvm-linear-raid01/</guid>
  <description><![CDATA[有时间再写🌑
REF 系统运维|Linux LVM简明教程
(1条消息) Linux | LVM | 对比三种逻辑卷（Logic Volume）_Hello Hunk的博客-CSDN博客]]></description>
</item>
<item>
  <title>PVE Souce</title>
  <link>/posts/2023-07-30-pve/</link>
  <pubDate>Sun, 30 Jul 2023 13:38:05 &#43;0000</pubDate>
  <author>Cyan</author>
  <guid>/posts/2023-07-30-pve/</guid>
  <description><![CDATA[PVE &amp; Debian pve 7.3.3基于debian 11 bullseye
APT源 1 sudo apt install apt-transport-https ca-certificates 修改souces.list
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free # deb https://mirrors.]]></description>
</item>
</channel>
</rss>
