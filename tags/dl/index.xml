<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>DL - 标签 - Cyan&#39;s Notebook</title>
    <link>/tags/dl/</link>
    <description>DL - 标签 - Cyan&#39;s Notebook</description>
    <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Tue, 05 Apr 2022 22:43:07 &#43;0800</lastBuildDate><atom:link href="/tags/dl/" rel="self" type="application/rss+xml" /><item>
  <title>Optimizer</title>
  <link>/posts/optimizer/</link>
  <pubDate>Tue, 05 Apr 2022 22:43:07 &#43;0800</pubDate>
  <author>Cyan</author>
  <guid>/posts/optimizer/</guid>
  <description><![CDATA[前言 参考的资料和自己在进行炼丹（姑且这么称作）的时候，经常使用的是Adam，在尝试了learning_rate, schedule等方法（无果，大概…）后，突然想到要去了解一下其中的原理。
资料 一个框架看懂优化算法之异同 SGD/AdaGrad/Adam - 知乎 (zhihu.com)
Adam那么棒，为什么还对SGD念念不忘 (2)—— Adam的两宗罪 - 知乎 (zhihu.com)
Adam那么棒，为什么还对SGD念念不忘 (3)—— 优化算法的选择与使用策略 - 知乎 (zhihu.com)
综述 Adam自带优化，会调整learning_rate（所以自己再用schedule, 微调learning_rate貌似没啥用了……
大神都用SGD手调参数]]></description>
</item>
<item>
  <title>tensorflow1、2设置不占满显存</title>
  <link>/posts/tf_mem/</link>
  <pubDate>Thu, 16 Sep 2021 18:46:00 &#43;0800</pubDate>
  <author>Cyan</author>
  <guid>/posts/tf_mem/</guid>
  <description><![CDATA[tensorflow的默认调度策略是吃满显存，如果我们模型较小、显存较大、占用率不足40%（自己估计的）时，是可以利用GPU同时训练两个模型的。
tensorflow 1 1 2 3 config = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config) tensorflow 2 1 2 3 gpus = tf.config.list_physical_devices(device_type=&#39;GPU&#39;) for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) ]]></description>
</item>
</channel>
</rss>
