<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>DL - 标签 - Cyan&#39;s Notebook</title>
    <link>/tags/dl/</link>
    <description>DL - 标签 - Cyan&#39;s Notebook</description>
    <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Tue, 05 Apr 2022 22:43:07 &#43;0800</lastBuildDate><atom:link href="/tags/dl/" rel="self" type="application/rss+xml" /><item>
  <title>Optimizer</title>
  <link>/posts/optimizer/</link>
  <pubDate>Tue, 05 Apr 2022 22:43:07 &#43;0800</pubDate>
  <author>Cyan</author>
  <guid>/posts/optimizer/</guid>
  <description><![CDATA[<h2 id="前言">前言</h2>
<p>参考的资料和自己在进行炼丹（姑且这么称作）的时候，经常使用的是Adam，在尝试了learning_rate, schedule等方法（无果，大概…）后，突然想到要去了解一下其中的原理。</p>]]></description>
</item>
<item>
  <title>tensorflow1、2设置不占满显存</title>
  <link>/posts/tf-gpu-memory/</link>
  <pubDate>Thu, 16 Sep 2021 18:46:00 &#43;0800</pubDate>
  <author>Cyan</author>
  <guid>/posts/tf-gpu-memory/</guid>
  <description><![CDATA[<p>tensorflow的默认调度策略是吃满显存，如果我们模型较小、显存较大、占用率不足40%（自己估计的）时，是可以利用GPU同时训练两个模型的。</p>]]></description>
</item>
</channel>
</rss>
